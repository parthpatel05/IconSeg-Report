{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization, ReLU, MaxPooling2D, Concatenate, Add, Conv2D, Subtract, Multiply, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import MeanIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with RGF Khush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_between_segmentation_imgs(y_hat_n, y_n):\n",
    "    pass\n",
    "\n",
    "def residual_guided_fusion(input_shape_rgb, input_shape_depth, y_n, filters, num_classes):\n",
    "    rgb_input = Input(shape=input_shape_rgb)\n",
    "    depth_input = Input(shape=input_shape_depth)\n",
    "\n",
    "    # RGB PATH\n",
    "    # generate RGB predicted mask y_hat_n through a 1 × 1 convolutional layer\n",
    "    y_hat_n = Conv2D(filters, kernel_size=(1, 1), padding='same')(rgb_input)\n",
    "    loss_n = calculate_loss_between_segmentation_imgs(y_hat_n, y_n)\n",
    "    y_res = Subtract()([y_n, y_hat_n])\n",
    "\n",
    "\n",
    "    # DEPTH PATH\n",
    "    # we subtract the RGB feature maps with depth feature maps by element-wise subtraction to get the difference between them. \n",
    "    difference_maps = Subtract()([depth_input, rgb_input])\n",
    "    # The channel of the different features is adjusted to the number of classes through a 1 × 1 convolution.\n",
    "    depth_conv = Conv2D(num_classes, kernel_size=(1, 1), padding='same')(difference_maps)\n",
    "    skip = depth_conv\n",
    "\n",
    "    # Then a residual unit with a 3 × 3convolution is used to generate the predicted residual mask y_hat_nres\n",
    "    depth_conv = Conv2D(filters, kernel_size=(3, 3), padding='same')(depth_conv)\n",
    "    y_hat_res = Add()([depth_conv, skip])\n",
    "\n",
    "    loss_res = calculate_loss_between_segmentation_imgs(y_hat_res, y_res)\n",
    "\n",
    "    # The channel of y_hat_res is adjusted to that of the RGB feature maps by a 1 ×1 convolution and result is fused with the RGB feature maps through an element-wise multiplication\n",
    "    channels_rgb = input_shape_rgb[-1]\n",
    "    y_hat_res_conv = Conv2D(channels_rgb, kernel_size=(1, 1), padding='same')(y_res)\n",
    "    combined_path = Multiply()([y_hat_res_conv, rgb_input])\n",
    "\n",
    "    stacked = Concatenate()([combined_path, rgb_input, y_hat_res_conv])\n",
    "\n",
    "    return Conv2D(filters, kernel_size=(3, 3), padding='same')(stacked)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGF Parth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_between_segmentation_imgs(y_hat_n, y_n):\n",
    "    pass\n",
    "\n",
    "def residual_guided_fusion(rgb_input, depth_input, filters, y_n=None, num_classes=19):\n",
    "    input_shape_rgb = rgb_input.shape\n",
    "    input_shape_rgb = depth_input.shape\n",
    "\n",
    "    # RGB PATH\n",
    "    # generate RGB predicted mask y_hat_n through a 1 × 1 convolutional layer\n",
    "    y_hat_n = Conv2D(filters, kernel_size=(1, 1), padding='same')(rgb_input)\n",
    "    # loss_n = calculate_loss_between_segmentation_imgs(y_hat_n, y_n)\n",
    "    # y_res = Subtract()([y_n, y_hat_n])\n",
    "\n",
    "\n",
    "    # DEPTH PATH\n",
    "    # we subtract the RGB feature maps with depth feature maps by element-wise subtraction to get the difference between them. \n",
    "    difference_maps = Subtract()([depth_input, rgb_input])\n",
    "    # The channel of the different features is adjusted to the number of classes through a 1 × 1 convolution.\n",
    "    depth_conv = Conv2D(filters, kernel_size=(1, 1), padding='same')(difference_maps)\n",
    "    skip = depth_conv\n",
    "\n",
    "    # Then a residual unit with a 3 × 3convolution is used to generate the predicted residual mask y_hat_nres\n",
    "    depth_conv = Conv2D(filters, kernel_size=(3, 3), padding='same')(depth_conv)\n",
    "    y_hat_res = Add()([depth_conv, skip])\n",
    "\n",
    "    # loss_res = calculate_loss_between_segmentation_imgs(y_hat_res, y_res)\n",
    "\n",
    "    # The channel of y_hat_res is adjusted to that of the RGB feature maps by a 1 ×1 convolution and result is fused with the RGB feature maps through an element-wise multiplication\n",
    "    channels_rgb = input_shape_rgb[-1]\n",
    "    y_hat_res_conv = Conv2D(channels_rgb, kernel_size=(1, 1), padding='same')(y_hat_res)\n",
    "    combined_path = Multiply()([y_hat_res_conv, rgb_input])\n",
    "\n",
    "    stacked = Concatenate()([combined_path, rgb_input, y_hat_res_conv])\n",
    "\n",
    "    return Conv2D(filters, kernel_size=(3, 3), padding='same')(stacked)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encoder_block(inputs, filters):\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    return x, x\n",
    "\n",
    "def decoder_block(inputs, skip_connection, filters, skip=True):\n",
    "    x = Conv2DTranspose(filters, kernel_size=(2, 2), strides=(2, 2), padding='same')(inputs)\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    if skip:\n",
    "        if x.shape[1] != skip_connection.shape[1] or x.shape[2] != skip_connection.shape[2]:\n",
    "            skip_connection = Conv2D(filters, kernel_size=(1, 1), padding='same')(skip_connection)\n",
    "        x = Concatenate()([x, skip_connection])  # Skip connection\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_shape_rgb, input_shape_depth):\n",
    "    rgb_input = Input(shape=input_shape_rgb)\n",
    "    depth_input = Input(shape=input_shape_depth)\n",
    "\n",
    "    # Encoder for RGB\n",
    "    rgb_enc1, rgb_skip1 = encoder_block(rgb_input, 32)\n",
    "    rgb_enc2, rgb_skip2 = encoder_block(rgb_enc1, 64)\n",
    "    rgb_enc3, rgb_skip3 = encoder_block(rgb_enc2, 128)\n",
    "\n",
    "    # Encoder for Depth\n",
    "    depth_enc1, depth_skip1 = encoder_block(depth_input, 32)\n",
    "    depth_enc2, depth_skip2 = encoder_block(depth_enc1, 64)\n",
    "    depth_enc3, depth_skip3 = encoder_block(depth_enc2, 128)\n",
    "\n",
    "    # Decoder for Depth\n",
    "    depth_dec3 = decoder_block(depth_enc3, depth_skip2, 128)\n",
    "    depth_dec2 = decoder_block(depth_dec3, depth_skip1, 64)\n",
    "    depth_dec1 = decoder_block(depth_dec2, None, 32, False)\n",
    "\n",
    "    # Decoder for RGB\n",
    "    rgb_dec3 = decoder_block(rgb_enc3, rgb_skip2, 128)\n",
    "    rgb_dec3 = Add()([rgb_dec3, depth_dec3])\n",
    "\n",
    "    rgb_dec2 = decoder_block(rgb_dec3, rgb_skip1, 64)\n",
    "    rgb_dec2 = Add()([rgb_dec2, depth_dec2])\n",
    "\n",
    "    rgb_dec1 = decoder_block(rgb_dec2, None, 32, False)\n",
    "    rgb_dec1 = Add()([rgb_dec1, depth_dec1])\n",
    "\n",
    "    # rgf module\n",
    "    rgf_1 = residual_guided_fusion(rgb_dec1, depth_dec1, 32)\n",
    "\n",
    "    # Final output layer for RGB\n",
    "    rgb_output = Conv2D(3, kernel_size=(1, 1), activation='sigmoid')(rgb_dec1)\n",
    "    \n",
    "# (input_shape_rgb, input_shape_depth, y_n, filters, num_classes):\n",
    "    # Create model\n",
    "    model = Model(inputs=[rgb_input, depth_input], outputs=rgb_output)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape_rgb = (256, 256, 3)\n",
    "input_shape_depth = (256, 256, 1)\n",
    "model = build_model(input_shape_rgb, input_shape_depth)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=[MeanIoU(num_classes=19)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import os\n",
    "\n",
    "# Define a custom data generator class to load RGB, depth, and label images\n",
    "class ImageDataGeneratorWithDepth(Sequence):\n",
    "    def __init__(self, image_dir, depth_dir, label_dir, batch_size, image_size, shuffle=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the number of batches per epoch\n",
    "        return int(np.floor(len(self.image_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the batch of data at the given index\n",
    "        batch_files = self.image_files[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        rgb_images = []\n",
    "        depth_images = []\n",
    "        label_images = []\n",
    "        \n",
    "        for file in batch_files:\n",
    "            # Load RGB image\n",
    "            rgb_path = os.path.join(self.image_dir, file)\n",
    "            rgb_img = load_img(rgb_path, target_size=self.image_size)\n",
    "            rgb_img = img_to_array(rgb_img) / 255.0  # Normalize to [0, 1]\n",
    "            \n",
    "            # Load depth image\n",
    "            depth_path = os.path.join(self.depth_dir, file)\n",
    "            depth_img = load_img(depth_path, target_size=self.image_size, color_mode='grayscale')\n",
    "            depth_img = img_to_array(depth_img) / 255.0  # Normalize to [0, 1]\n",
    "            \n",
    "            # Load label image\n",
    "            label_path = os.path.join(self.label_dir, file)\n",
    "            label_img = load_img(label_path, target_size=self.image_size, color_mode='grayscale')\n",
    "            label_img = img_to_array(label_img) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "            # Append images to the lists\n",
    "            rgb_images.append(rgb_img)\n",
    "            depth_images.append(depth_img)\n",
    "            label_images.append(label_img)\n",
    "        \n",
    "        # Stack images to create numpy arrays\n",
    "        X = np.concatenate([np.array(rgb_images), np.array(depth_images)], axis=-1)  # Concatenate RGB and depth\n",
    "        y = np.array(label_images)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle the files at the end of every epoch\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.image_files)\n",
    "\n",
    "# Define the parameters\n",
    "image_dir = 'cityscapes/train/image'\n",
    "depth_dir = 'cityscapes/train/depth'\n",
    "label_dir = 'cityscapes/train/label'\n",
    "image_size = (256, 256)  # Resize all images to 256x256\n",
    "batch_size = 32\n",
    "\n",
    "# Create the training data generator\n",
    "train_generator = ImageDataGeneratorWithDepth(image_dir, depth_dir, label_dir, batch_size, image_size)\n",
    "\n",
    "# Define the same generator for validation data\n",
    "val_image_dir = 'cityscapes/val/image'\n",
    "val_depth_dir = 'cityscapes/val/depth'\n",
    "val_label_dir = 'cityscapes/val/label'\n",
    "\n",
    "val_generator = ImageDataGeneratorWithDepth(val_image_dir, val_depth_dir, val_label_dir, batch_size, image_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x000001B8CCD6EB60>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the generators in Keras model training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m, in \u001b[0;36mImageDataGeneratorWithDepth.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m batch_files:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Load RGB image\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     rgb_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, file)\n\u001b[1;32m---> 33\u001b[0m     rgb_img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     rgb_img \u001b[38;5;241m=\u001b[39m img_to_array(rgb_img) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize to [0, 1]\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Load depth image\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3498\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3496\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m   3497\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[1;32m-> 3498\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x000001B8CCD6EB60>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the generators in Keras model training\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
