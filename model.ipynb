{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization, ReLU, MaxPooling2D, Concatenate, Add, Conv2D, Subtract, Multiply, concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with RGF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_between_segmentation_imgs(y_hat_n, y_n):\n",
    "    pass\n",
    "\n",
    "def residual_guided_fusion(input_shape_rgb, input_shape_depth, y_n, filters, num_classes):\n",
    "    rgb_input = Input(shape=input_shape_rgb)\n",
    "    depth_input = Input(shape=input_shape_depth)\n",
    "\n",
    "    # RGB PATH\n",
    "    # generate RGB predicted mask y_hat_n through a 1 × 1 convolutional layer\n",
    "    y_hat_n = Conv2D(filters, kernel_size=(1, 1), padding='same')(rgb_input)\n",
    "    loss_n = calculate_loss_between_segmentation_imgs(y_hat_n, y_n)\n",
    "    y_res = Subtract()([y_n, y_hat_n])\n",
    "\n",
    "\n",
    "    # DEPTH PATH\n",
    "    # we subtract the RGB feature maps with depth feature maps by element-wise subtraction to get the difference between them. \n",
    "    difference_maps = Subtract()([depth_input, rgb_input])\n",
    "    # The channel of the different features is adjusted to the number of classes through a 1 × 1 convolution.\n",
    "    depth_conv = Conv2D(num_classes, kernel_size=(1, 1), padding='same')(difference_maps)\n",
    "    skip = depth_conv\n",
    "\n",
    "    # Then a residual unit with a 3 × 3convolution is used to generate the predicted residual mask y_hat_nres\n",
    "    depth_conv = Conv2D(filters, kernel_size=(3, 3), padding='same')(depth_conv)\n",
    "    y_hat_res = Add()([depth_conv, skip])\n",
    "\n",
    "    loss_res = calculate_loss_between_segmentation_imgs(y_hat_res, y_res)\n",
    "\n",
    "    # The channel of y_hat_res is adjusted to that of the RGB feature maps by a 1 ×1 convolution and result is fused with the RGB feature maps through an element-wise multiplication\n",
    "    channels_rgb = input_shape_rgb[-1]\n",
    "    y_hat_res_conv = Conv2D(channels_rgb, kernel_size=(1, 1), padding='same')(y_res)\n",
    "    combined_path = Multiply()([y_hat_res_conv, rgb_input])\n",
    "\n",
    "    stacked = Concatenate()([combined_path, rgb_input, y_hat_res_conv])\n",
    "\n",
    "    return Conv2D(filters, kernel_size=(3, 3), padding='same')(stacked)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_between_segmentation_imgs(y_hat_n, y_n):\n",
    "    pass\n",
    "\n",
    "def residual_guided_fusion(rgb_input, depth_input, filters, y_n=None, num_classes=19):\n",
    "    input_shape_rgb = rgb_input.shape\n",
    "    input_shape_rgb = depth_input.shape\n",
    "\n",
    "    # RGB PATH\n",
    "    # generate RGB predicted mask y_hat_n through a 1 × 1 convolutional layer\n",
    "    y_hat_n = Conv2D(filters, kernel_size=(1, 1), padding='same')(rgb_input)\n",
    "    # loss_n = calculate_loss_between_segmentation_imgs(y_hat_n, y_n)\n",
    "    # y_res = Subtract()([y_n, y_hat_n])\n",
    "\n",
    "\n",
    "    # DEPTH PATH\n",
    "    # we subtract the RGB feature maps with depth feature maps by element-wise subtraction to get the difference between them. \n",
    "    difference_maps = Subtract()([depth_input, rgb_input])\n",
    "    # The channel of the different features is adjusted to the number of classes through a 1 × 1 convolution.\n",
    "    depth_conv = Conv2D(filters, kernel_size=(1, 1), padding='same')(difference_maps)\n",
    "    skip = depth_conv\n",
    "\n",
    "    # Then a residual unit with a 3 × 3convolution is used to generate the predicted residual mask y_hat_nres\n",
    "    depth_conv = Conv2D(filters, kernel_size=(3, 3), padding='same')(depth_conv)\n",
    "    y_hat_res = Add()([depth_conv, skip])\n",
    "\n",
    "    # loss_res = calculate_loss_between_segmentation_imgs(y_hat_res, y_res)\n",
    "\n",
    "    # The channel of y_hat_res is adjusted to that of the RGB feature maps by a 1 ×1 convolution and result is fused with the RGB feature maps through an element-wise multiplication\n",
    "    channels_rgb = input_shape_rgb[-1]\n",
    "    y_hat_res_conv = Conv2D(channels_rgb, kernel_size=(1, 1), padding='same')(y_hat_res)\n",
    "    combined_path = Multiply()([y_hat_res_conv, rgb_input])\n",
    "\n",
    "    stacked = Concatenate()([combined_path, rgb_input, y_hat_res_conv])\n",
    "\n",
    "    return Conv2D(filters, kernel_size=(3, 3), padding='same')(stacked)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encoder_block(inputs, filters):\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    return x, x\n",
    "\n",
    "def decoder_block(inputs, skip_connection, filters, skip=True):\n",
    "    x = Conv2DTranspose(filters, kernel_size=(2, 2), strides=(2, 2), padding='same')(inputs)\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    if skip:\n",
    "        if x.shape[1] != skip_connection.shape[1] or x.shape[2] != skip_connection.shape[2]:\n",
    "            skip_connection = Conv2D(filters, kernel_size=(1, 1), padding='same')(skip_connection)\n",
    "        x = Concatenate()([x, skip_connection])  # Skip connection\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_shape_rgb, input_shape_depth):\n",
    "    rgb_input = Input(shape=input_shape_rgb)\n",
    "    depth_input = Input(shape=input_shape_depth)\n",
    "\n",
    "    # Encoder for RGB\n",
    "    rgb_enc1, rgb_skip1 = encoder_block(rgb_input, 32)\n",
    "    rgb_enc2, rgb_skip2 = encoder_block(rgb_enc1, 64)\n",
    "    rgb_enc3, rgb_skip3 = encoder_block(rgb_enc2, 128)\n",
    "\n",
    "    # Encoder for Depth\n",
    "    depth_enc1, depth_skip1 = encoder_block(depth_input, 32)\n",
    "    depth_enc2, depth_skip2 = encoder_block(depth_enc1, 64)\n",
    "    depth_enc3, depth_skip3 = encoder_block(depth_enc2, 128)\n",
    "\n",
    "    # Decoder for Depth\n",
    "    depth_dec3 = decoder_block(depth_enc3, depth_skip2, 128)\n",
    "    depth_dec2 = decoder_block(depth_dec3, depth_skip1, 64)\n",
    "    depth_dec1 = decoder_block(depth_dec2, None, 32, False)\n",
    "\n",
    "    # Decoder for RGB\n",
    "    rgb_dec3 = decoder_block(rgb_enc3, rgb_skip2, 128)\n",
    "    rgb_dec3 = Add()([rgb_dec3, depth_dec3])\n",
    "\n",
    "    rgb_dec2 = decoder_block(rgb_dec3, rgb_skip1, 64)\n",
    "    rgb_dec2 = Add()([rgb_dec2, depth_dec2])\n",
    "\n",
    "    rgb_dec1 = decoder_block(rgb_dec2, None, 32, False)\n",
    "    rgb_dec1 = Add()([rgb_dec1, depth_dec1])\n",
    "\n",
    "    # rgf module\n",
    "    rgf_1 = residual_guided_fusion(rgb_dec1, depth_dec1, 32)\n",
    "\n",
    "    # Final output layer for RGB\n",
    "    rgb_output = Conv2D(3, kernel_size=(1, 1), activation='sigmoid')(rgb_dec1)\n",
    "    \n",
    "# (input_shape_rgb, input_shape_depth, y_n, filters, num_classes):\n",
    "    # Create model\n",
    "    model = Model(inputs=[rgb_input, depth_input], outputs=rgb_output)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape_rgb = (256, 256, 3)\n",
    "input_shape_depth = (256, 256, 1)\n",
    "model = build_model(input_shape_rgb, input_shape_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
